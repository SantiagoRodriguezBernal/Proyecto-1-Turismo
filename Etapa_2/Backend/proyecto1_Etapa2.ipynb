{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: num2words in c:\\users\\zeify\\appdata\\roaming\\python\\python311\\site-packages (0.5.13)\n",
      "Requirement already satisfied: docopt>=0.6.2 in c:\\users\\zeify\\appdata\\roaming\\python\\python311\\site-packages (from num2words) (0.6.2)\n",
      "Requirement already satisfied: langdetect in c:\\users\\zeify\\appdata\\roaming\\python\\python311\\site-packages (1.0.9)\n",
      "Requirement already satisfied: six in d:\\anacondanavigator\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Requirement already satisfied: stanza in c:\\users\\zeify\\appdata\\roaming\\python\\python311\\site-packages (1.8.1)\n",
      "Requirement already satisfied: emoji in c:\\users\\zeify\\appdata\\roaming\\python\\python311\\site-packages (from stanza) (2.11.0)\n",
      "Requirement already satisfied: numpy in d:\\anacondanavigator\\lib\\site-packages (from stanza) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in c:\\users\\zeify\\appdata\\roaming\\python\\python311\\site-packages (from stanza) (5.26.1)\n",
      "Requirement already satisfied: requests in d:\\anacondanavigator\\lib\\site-packages (from stanza) (2.31.0)\n",
      "Requirement already satisfied: networkx in d:\\anacondanavigator\\lib\\site-packages (from stanza) (3.1)\n",
      "Requirement already satisfied: toml in d:\\anacondanavigator\\lib\\site-packages (from stanza) (0.10.2)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\users\\zeify\\appdata\\roaming\\python\\python311\\site-packages (from stanza) (2.2.2)\n",
      "Requirement already satisfied: tqdm in d:\\anacondanavigator\\lib\\site-packages (from stanza) (4.65.0)\n",
      "Requirement already satisfied: filelock in d:\\anacondanavigator\\lib\\site-packages (from torch>=1.3.0->stanza) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\zeify\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.3.0->stanza) (4.11.0)\n",
      "Requirement already satisfied: sympy in d:\\anacondanavigator\\lib\\site-packages (from torch>=1.3.0->stanza) (1.12)\n",
      "Requirement already satisfied: jinja2 in d:\\anacondanavigator\\lib\\site-packages (from torch>=1.3.0->stanza) (3.1.3)\n",
      "Requirement already satisfied: fsspec in d:\\anacondanavigator\\lib\\site-packages (from torch>=1.3.0->stanza) (2023.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anacondanavigator\\lib\\site-packages (from requests->stanza) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anacondanavigator\\lib\\site-packages (from requests->stanza) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anacondanavigator\\lib\\site-packages (from requests->stanza) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anacondanavigator\\lib\\site-packages (from requests->stanza) (2024.2.2)\n",
      "Requirement already satisfied: colorama in d:\\anacondanavigator\\lib\\site-packages (from tqdm->stanza) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anacondanavigator\\lib\\site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\anacondanavigator\\lib\\site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "#Instalaciones\n",
    "\n",
    "#Libreria para remplazar numeros por palabras\n",
    "!pip install num2words\n",
    "#Instalar  pandas profiler\n",
    "#!pip install pandas-profiling==2.7.1\n",
    "#deteccion de lenguaje para eliminar entradas que no esten en español\n",
    "!pip install langdetect\n",
    "#Procesamientno de lenguaje natural en español\n",
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d1387eac80b4ae59265c2848e653956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-18 17:42:16 INFO: Downloaded file to C:\\Users\\zeify\\stanza_resources\\resources.json\n",
      "2024-04-18 17:42:16 INFO: Downloading default packages for language: es (Spanish) ...\n",
      "2024-04-18 17:42:18 INFO: File exists: C:\\Users\\zeify\\stanza_resources\\es\\default.zip\n",
      "2024-04-18 17:42:24 INFO: Finished downloading models and saved to C:\\Users\\zeify\\stanza_resources\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zeify\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Imports para procesamiento de texto\n",
    "\n",
    "#tokenizacion y lematizacion\n",
    "import stanza\n",
    "#Para integrar pasos de la limpieza adicionales\n",
    "from stanza.pipeline.processor import Processor, register_processor\n",
    "#paquete español\n",
    "stanza.download('es')\n",
    "\n",
    "#Para manejo de numeros, singluares, plurarles en lenguaje\n",
    "from num2words import num2words\n",
    "#Deteccion de lenguaje\n",
    "from langdetect import detect\n",
    "# librería Natural Language Toolkit, usada para trabajar con textos\n",
    "import nltk\n",
    "# Punkt permite separar un texto en frases.\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Operaciones con expresiones regulares y unicode\n",
    "import re, string, unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zeify\\AppData\\Local\\Temp\\ipykernel_7572\\122007551.py:5: DeprecationWarning: `import pandas_profiling` is going to be deprecated by April 1st. Please use `import ydata_profiling` instead.\n",
      "  from pandas_profiling import ProfileReport\n"
     ]
    }
   ],
   "source": [
    "#Imports generales para analisis de datos y ML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from pandas_profiling import ProfileReport\n",
    "import statistics\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Lectura de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Muy buena atención y aclaración de dudas por p...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Buen hotel si están obligados a estar cerca de...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Es un lugar muy lindo para fotografías, visite...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abusados con la factura de alimentos siempre s...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tuvimos un par de personas en el grupo que rea...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Class\n",
       "0  Muy buena atención y aclaración de dudas por p...      5\n",
       "1  Buen hotel si están obligados a estar cerca de...      3\n",
       "2  Es un lugar muy lindo para fotografías, visite...      5\n",
       "3  Abusados con la factura de alimentos siempre s...      3\n",
       "4  Tuvimos un par de personas en el grupo que rea...      3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lectura de los datos.\n",
    "df_turi = pd.read_csv('tipo2_entrenamiento_estudiantes.csv', sep=',', encoding='utf-8')\n",
    "\n",
    "df_turi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construcción del pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def init(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "\n",
    "        X_processed = self.customPreprocessing(X)\n",
    "        #Retornar los datos\n",
    "        return X_processed\n",
    "    \n",
    "    #Remplaza los numeros por su representacion en palabras\n",
    "    def replace_numbers(self, words):\n",
    "        \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            if word.isdigit():\n",
    "                new_word = num2words(word, lang='es')\n",
    "                new_words.append(new_word)\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "        return new_words\n",
    "    #Remueve todo caracter no latino (conserva espacios y numeros)\n",
    "    def remove_nonlatin(self, words):\n",
    "      new_words = []\n",
    "      for word in words:\n",
    "        new_word = ''\n",
    "        for ch in word:\n",
    "          if unicodedata.name(ch).startswith(('LATIN', 'DIGIT', 'SPACE')):\n",
    "            new_word += ch\n",
    "        new_words.append(new_word)\n",
    "      return new_words\n",
    "\n",
    "    #Remueve palabras comunes que no aportan informacion\n",
    "    def remove_stopwords(self, words):\n",
    "        \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        s = set(stopwords.words('spanish'))\n",
    "        for word in words:\n",
    "            if word not in s:\n",
    "                new_words.append(word)\n",
    "        return new_words\n",
    "\n",
    "    #Remueve puntuacion\n",
    "    def remove_punctuation(self, words):\n",
    "        \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "        new_words = ''\n",
    "        for word in words:\n",
    "                new_words += re.sub(r'[^\\w\\s]', ' ', word)\n",
    "        return new_words\n",
    "\n",
    "     #Procesamiento de cada review usando stanza\n",
    "    def tokenLemma(self, data):\n",
    "      data['words'] = data['Review'].apply(self.remove_punctuation)\n",
    "      #Creamos un pipeline para tokenizacion y lematizacion\n",
    "      nlp = stanza.Pipeline('es', processors = 'tokenize,mwt,pos,lemma', use_gpu=True)\n",
    "      in_docs = [stanza.Document([], text=d) for d in data.words]\n",
    "      return nlp(in_docs)\n",
    "\n",
    "    #Funcion secundaria para procesar cada token\n",
    "    def procesamientoPalabras(self, words):\n",
    "        words = self.remove_nonlatin(words)\n",
    "        words = self.replace_numbers(words)\n",
    "        words = self.remove_stopwords(words)\n",
    "        return words\n",
    "\n",
    "    #Funcion principal para el pre-procesamiento\n",
    "    def customPreprocessing(self, data):\n",
    "        out_docs = self.tokenLemma(data)\n",
    "        palabras = []\n",
    "\n",
    "        for doc in out_docs:\n",
    "            reviewAct = []\n",
    "            for sentence in doc.sentences:\n",
    "              for word in sentence.words:\n",
    "                if(word.pos != 'PUNCT' and word.pos != 'SYM'):\n",
    "                  reviewAct.append(word.lemma.lower())\n",
    "            palabras.append(reviewAct)\n",
    "        \n",
    "        data['words'] = palabras\n",
    "        data['words'] = data['words'].apply(self.procesamientoPalabras)\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import style\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import RepeatedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRegression(BaseEstimator, TransformerMixin):\n",
    "    def init(self):\n",
    "        self.model = None\n",
    "        self.params = None\n",
    "        self.accuracy = None\n",
    "        self.vec = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "                \n",
    "        X['words'] = X['words'].apply(lambda x: ' '.join(map(str, x)))\n",
    "        \n",
    "        #Separación de los datos en conjunto de test y train\n",
    "        X = X.drop('Review', axis = 1)\n",
    "        df_train, df_test = sklearn.model_selection.train_test_split(X, test_size=0.2, random_state=0)\n",
    "\n",
    "        X_train = df_train['words']\n",
    "        y_train = df_train['Class']\n",
    "\n",
    "        X_test = df_test['words']\n",
    "        y_test = df_test['Class']\n",
    "        \n",
    "        #Vectorizar los datos con Tfid\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        train_vectors = vectorizer.fit_transform(X_train)\n",
    "        test_vectors = vectorizer.transform(X_test)\n",
    "        \n",
    "        self.vec = vectorizer\n",
    "        \n",
    "        parameters = {\n",
    "            'penalty' : ['l1','l2', 'elasticnet', None], \n",
    "            'C'       : np.logspace(-10,10,3),\n",
    "            'solver'  : ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "        }\n",
    "        \n",
    "        metrica = RepeatedKFold(n_splits=20, n_repeats= 10, random_state=0)\n",
    "\n",
    "        logreg = LogisticRegression()\n",
    "        modelo = GridSearchCV(logreg, param_grid = parameters, scoring='accuracy', cv=metrica, n_jobs=-1)  \n",
    "        \n",
    "        modelo.fit(train_vectors,y_train)\n",
    "        self.params = modelo.best_params_\n",
    "        self.accuracy = modelo.best_score_\n",
    "        modelo_optimo = modelo.best_estimator_\n",
    "                \n",
    "        self.model = modelo_optimo\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):        \n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_preprocessor = CustomPreprocessor()\n",
    "custom_regression = CustomRegression()\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"processing\", custom_preprocessor),\n",
    "        (\"model\", custom_regression)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-18 18:18:43 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac56c90712fd41dd9e34840b0720824d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-18 18:18:43 INFO: Downloaded file to C:\\Users\\zeify\\stanza_resources\\resources.json\n",
      "2024-04-18 18:18:44 INFO: Loading these models for language: es (Spanish):\n",
      "===============================\n",
      "| Processor | Package         |\n",
      "-------------------------------\n",
      "| tokenize  | ancora          |\n",
      "| mwt       | ancora          |\n",
      "| pos       | ancora_charlm   |\n",
      "| lemma     | ancora_nocharlm |\n",
      "===============================\n",
      "\n",
      "2024-04-18 18:18:44 WARNING: GPU requested, but is not available!\n",
      "2024-04-18 18:18:44 INFO: Using device: cpu\n",
      "2024-04-18 18:18:44 INFO: Loading: tokenize\n",
      "2024-04-18 18:18:50 INFO: Loading: mwt\n",
      "2024-04-18 18:18:50 INFO: Loading: pos\n",
      "2024-04-18 18:18:51 INFO: Loading: lemma\n",
      "2024-04-18 18:18:51 INFO: Done loading processors!\n",
      "d:\\AnacondaNavigator\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "3600 fits failed out of a total of 7200.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "600 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\AnacondaNavigator\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\AnacondaNavigator\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\AnacondaNavigator\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "600 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\AnacondaNavigator\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\AnacondaNavigator\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\AnacondaNavigator\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "600 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\AnacondaNavigator\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\AnacondaNavigator\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\AnacondaNavigator\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "600 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\AnacondaNavigator\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\AnacondaNavigator\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\AnacondaNavigator\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "600 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\AnacondaNavigator\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\AnacondaNavigator\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\AnacondaNavigator\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "600 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\AnacondaNavigator\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\AnacondaNavigator\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1216, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "                                                ^^^^^^^^^^^^^^^\n",
      "  File \"d:\\AnacondaNavigator\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\AnacondaNavigator\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "d:\\AnacondaNavigator\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.09936508 0.30428571 0.30428571 0.30428571\n",
      "        nan        nan        nan 0.43820635 0.44874603        nan\n",
      "        nan        nan 0.4781746  0.49233333 0.49226984 0.49053968\n",
      "        nan        nan        nan 0.43820635 0.44874603        nan\n",
      "        nan        nan 0.37473016 0.43784127 0.44871429 0.42850794\n",
      "        nan        nan        nan 0.43820635 0.44874603        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;processing&#x27;, CustomPreprocessor()),\n",
       "                (&#x27;model&#x27;, CustomRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;processing&#x27;, CustomPreprocessor()),\n",
       "                (&#x27;model&#x27;, CustomRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CustomPreprocessor</label><div class=\"sk-toggleable__content\"><pre>CustomPreprocessor()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CustomRegression</label><div class=\"sk-toggleable__content\"><pre>CustomRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('processing', CustomPreprocessor()),\n",
       "                ('model', CustomRegression())])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ajusta el modelo en tus datos transformados\n",
    "pipeline.fit(df_turi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = pipeline[\"model\"].model\n",
    "vectorizer = pipeline[\"model\"].vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testear el funcionamiento del pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se cargan los datos no etiquetados\n",
    "df_test = pd.read_csv(\"df_procesado_predecir.csv\", sep = \";\", encoding='utf-8')\n",
    "import ast\n",
    "df_test['words'] = df_test['words'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertimos los tokens nuevamente en strings\n",
    "df_test['words'] = df_test['words'].apply(lambda x: ' '.join(map(str, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separar datos para predecir\n",
    "df_test = df_test.drop('Review', axis = 1)\n",
    "\n",
    "X_predict = df_test['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       primero noche encontrar habitación nido cinco ...\n",
       "1       calle catedral platillo tradicional tipo gourm...\n",
       "2       porción miserable agua sabor cloro distraído m...\n",
       "3       cartagena encantar ciudad colonial haber visit...\n",
       "4       ir ilusion disfrutar espectaculo luz sonido ve...\n",
       "                              ...                        \n",
       "1745    subir funicular bajar teleferico ser buen expe...\n",
       "1746    gente esperar lugar central habana fiesta naci...\n",
       "1747    excelente hotel alberca niño mejor atención me...\n",
       "1748    detener bocado puesta sol haber nadie restaura...\n",
       "1749    tener agua mantenimiento desastre mochila baño...\n",
       "Name: words, Length: 1750, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorizar los datos con Tfid\n",
    "vectors = vectorizer.transform(X_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 5, 2, ..., 5, 3, 1], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predecir los datos y guardarlos en el archivo\n",
    "\n",
    "predict = reg.predict(vectors)\n",
    "predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Persistencia del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ModeloReview.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"ModeloReview.joblib\"\n",
    "dump(pipeline, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
